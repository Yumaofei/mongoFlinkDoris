<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>

<!--1、元数据相关 -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hn07:3306/hive?useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>hive</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>tT#ABC@123!</value>
    </property>
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/data/hadoop/hive/warehouse</value>
    </property>
   <!-- Hive元数据存储的验证,因为3.x的hive推荐使用mysql5.8以上版本-->
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>
    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>
		<property>
        <name>hive.metastore.uris</name>
        <value>thrift://hn07:9083</value>
        <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
    </property>
    <!--指定元数据序列化与反序列化器-->
    <property>
	<name>metastore.storage.schema.reader.impl</name>
	<value>org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader</value>
    </property>


<!--2、hiveserver2服务相关 -->
    <property>
         <name>hive.server2.thrift.port</name>
         <value>10000</value>
    </property>
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>hn07</value>
    </property>
    <property>
        <name>hive.server2.sleep.interval.between.start.attempts</name>
        <value>1s</value>
    </property>


<!--堆内存设置 -->	
    <property>
	<name>hive.server2.heapsize</name>
	<value>32768</value>
    </property>
    <property>
	<name>hive.metastore.heapsize</name>
	<value>32768</value>
    </property>

<!--使用YARN作为资源管理器 -->
<property>
  <name>hive.server2.resource.manager</name>
  <value>yarn</value>
</property>

<property>
  <name>hive.metastore.resource.manager</name>
  <value>yarn</value>
</property>

	
    <!--控制HiveServer2可用于处理客户端请求的最小线程数，默认值为 5。可以设置为 10 或更高，确保多个Hive客户端连接时，HiveServer2 可以分配足够的线程。 -->
    <property>
	<name>hive.server2.thrift.min.worker.threads</name>
	<value>10</value>
	<description>Minimum number of Thrift worker threads</description>
    </property>
    <!--控制HiveServer2可用于处理客户端请求的最大线程数。默认值为 500。可设置为 1000 或更高。对于小规模集群，您可以根据需要将其设置为较低的值。 -->
    <property>
	<name>hive.server2.thrift.max.worker.threads</name>
	<value>3000</value>
	<description>Maximum number of Thrift worker threads</description>
    </property>
    <property>
	<name>hive.server2.idle.session.timeout</name>
	<value>7200s</value>
	<description>
      Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec), which is msec if not specified.
      Session will be closed when it's not accessed for this duration, which can be disabled by setting to zero or negative value.
	</description>
    </property>
    <property>
	<name>hive.server2.idle.operation.timeout</name>
	<value>7200s</value>
	<description>
        Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec), which is msec if not specified.
        Operation will be closed when it's not accessed for this duration of time, which can be disabled by setting to zero value.
        With positive value, it's checked for operations in terminal state only (FINISHED, CANCELED, CLOSED, ERROR).
        With negative value, it's checked for all of the operations regardless of state.
	</description>
    </property>
<!--决定hiveserver2的长轮询超时时间。默认值5000ms,如果查询通常需要很长时间才能完成。需要增加这个值-->
<!--  <property>
    <name>hive.server2.long.polling.timeout</name>
    <value>50000ms</value>
    <description>
      Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec), which is msec if not specified.
      Time that HiveServer2 will wait before responding to asynchronous calls that use long polling
    </description>
  </property> -->


<!--3、hive on spark调优-->
    <!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）-->
    <property>
	<name>spark.yarn.jars</name>
	<value>hdfs://tTCluster/spark/jars/*</value>
    </property> 
    <property>
	<name>hive.execution.engine</name>
	<value>spark</value>
    </property>
    <property>
	<name>hive.spark.client.connect.timeout</name>
	<value>60000ms</value>
	<description>
	Expects a time value with unit (d/day, h/hour, m/min, s/sec, ms/msec, us/usec, ns/nsec), which is msec if not specified.
	Timeout for remote Spark driver in connecting back to Hive client.
	</description>
    </property> 


<!--4、其他开发相关调优-->
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
	
    <!--动态分区-->
    <property>
        <name>hive.exec.dynamic.partition</name>
        <value>true</value>
        <description>Whether or not to allow dynamic partitions in DML/DDL.</description>
    </property>
    <property>
        <name>hive.exec.dynamic.partition.mode</name>
        <value>nonstrict</value>
        <description>
          In strict mode, the user must specify at least one static partition
          in case the user accidentally overwrites all partitions.
          In nonstrict mode all partitions are allowed to be dynamic.
        </description>
    </property>
    <property>
        <name>hive.exec.max.dynamic.partitions</name>
        <value>2000</value>
        <description>Maximum number of dynamic partitions allowed to be created in total.</description>
    </property>
    <property>
        <name>hive.exec.max.dynamic.partitions.pernode</name>
        <value>2000</value>
        <description>Maximum number of dynamic partitions allowed to be created in each mapper/reducer node.</description>
	</property>
   
    <!--合并map/reduce任务输出的小文件 -->
    <property>
	<name>hive.merge.mapredfiles</name>
        <value>true</value>
    </property>
    <property>
	<name>hive.merge.sparkfiles</name>
	<value>true</value>
	<description>Merge small files at the end of a Spark DAG Transformation</description>
    </property>

    <!-- 要使用的 reducer 的最大数量。如果配置参数 Hive Reduce Tasks 为负，则 Hive 会将 reducer 数量限制为此参数的值。集群的默认值是：1009，修改后的值为250。-->
    <property>
	<name>hive.exec.reducers.max</name>
	<value>250</value>
	<description>
	max number of reducers will be used. If the one specified in the configuration parameter mapred.reduce.tasks is
	negative, Hive will use this one as the max number of reducers when automatically determine number of reducers.
	</description>
    </property>

    <!--默认为false。是否启用作业并发执行，如果启用，一些不冲突的task可以同时执行，例如在join语句中访问不同表获取数据的task -->
    <property>
	<name>hive.exec.parallel</name>
	<value>true</value>
	<description>Whether to execute jobs in parallel</description>
    </property>
    <property>
	<name>hive.exec.parallel.thread.number</name>
	<value>16</value>
    </property>
    <!-- 严格类型安全 -->
    <property>
        <name>hive.strict.checks.type.safety</name>
        <value>false</value>
    </property>
    <!-- 日志相关 -->
    <property>
        <name>hive.server2.logging.operation.enabled</name>
        <value>true</value>
        <description>Enable logging of HiveServer2 operations</description>
    </property>

    <property>
        <name>hive.server2.logging.operation.log.location</name>
        <value>/home/hadoop/apache-hive-3.1.3-bin/logs/hiveserver2/operation</value>
        <description>Location of HiveServer2 operation logs</description>
    </property>

    <property>
        <name>hive.server2.logging.operation.log.level</name>
        <value>INFO</value>
        <description>Logging level for HiveServer2 operation logs</description>
    </property>

    <property>
        <name>hive.server2.logging.query.enabled</name>
        <value>true</value>
        <description>Enable logging of HiveServer2 queries</description>
    </property>

    <property>
        <name>hive.server2.logging.query.log.location</name>
        <value>/home/hadoop/apache-hive-3.1.3-bin/logs/hiveserver2/query</value>
        <description>Location of HiveServer2 query logs</description>
    </property>

    <property>
        <name>hive.server2.logging.query.log.level</name>
        <value>INFO</value>
        <description>Logging level for HiveServer2 query logs</description>
    </property>	
</configuration>
